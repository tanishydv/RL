{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW80/H8eGeIa53BSlff5So",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashubhardwaj001/RL/blob/main/Exp_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1jT1IFw69SmU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#R matrix\n",
        "R = np.matrix([[-1, -1, -1, -1, 0, -1],\n",
        "               [-1, -1, -1 ,0, -1, 100],\n",
        "               [-1, -1, -1, 0, -1, -1],\n",
        "               [-1, 0, 0, -1, 0, -1],\n",
        "               [-1, 0, 0, -1, -1, 100],\n",
        "               [-1, 0, -1, -1, 0, 100]])\n",
        "R\n",
        "matrix([[ -1,  -1,  -1,  -1,   0,  -1],\n",
        "        [ -1,  -1,  -1,   0,  -1, 100],\n",
        "        [ -1,  -1,  -1,   0,  -1,  -1],\n",
        "        [ -1,   0,   0,  -1,   0,  -1],\n",
        "        [ -1,   0,   0,  -1,  -1, 100],\n",
        "        [ -1,   0,  -1,  -1,   0, 100]])\n",
        "#Q Matrix\n",
        "Q = np.matrix(np.zeros([6,6]))\n",
        "Q\n",
        "matrix([[0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.]])\n",
        "gamma = 0.8\n",
        "initial_state = 1\n",
        "def available_actions(state):\n",
        "  current_state_row=R[state,]\n",
        "  av_act = np.where(current_state_row>=0)[1]\n",
        "  return av_act\n",
        "\n",
        "available_act  = available_actions(initial_state)\n",
        "available_act\n",
        "array([3, 5])\n",
        "R\n",
        "matrix([[ -1,  -1,  -1,  -1,   0,  -1],\n",
        "        [ -1,  -1,  -1,   0,  -1, 100],\n",
        "        [ -1,  -1,  -1,   0,  -1,  -1],\n",
        "        [ -1,   0,   0,  -1,   0,  -1],\n",
        "        [ -1,   0,   0,  -1,  -1, 100],\n",
        "        [ -1,   0,  -1,  -1,   0, 100]])\n",
        "#this function chooses at random which action to be performend with\n",
        "def sample_next_action(available_actions_range):\n",
        "  next_action=int(np.random.choice(available_act,1))\n",
        "  return next_action\n",
        "#sample next action to be performed\n",
        "action=sample_next_action(available_act)\n",
        "action\n",
        "def update(current_state, action, game):\n",
        "  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
        "\n",
        "  if max_index.shape[0]>1:\n",
        "    max_index=int(np.random.choice(max_index,size=1))\n",
        "  else:\n",
        "    max_index=int(max_index)\n",
        "  max_value = Q[action, max_index]\n",
        "\n",
        "  #Q learning formula\n",
        "  Q[current_state,action] = R[current_state,action] + gamma*max_value\n",
        "\n",
        "\n",
        "update(initial_state, action, gamma)\n",
        "\n",
        "\n",
        "\n",
        "Q\n",
        "matrix([[0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.],\n",
        "        [0., 0., 0., 0., 0., 0.]])\n",
        "#Training\n",
        "#Train over 10,000 iterations\n",
        "for i in range(10000):\n",
        "  current_state = np.random.randint(0, int(Q.shape[0]))\n",
        "  available_act = available_actions(current_state)\n",
        "  action = sample_next_action(available_act)\n",
        "  update(current_state, action, gamma)\n",
        "print(\"Trained Q matrix\")\n",
        "print(Q/np.max(Q)*100)\n",
        "Trained Q matrix\n",
        "[[  0.    0.    0.    0.   80.    0. ]\n",
        " [  0.    0.    0.   64.    0.  100. ]\n",
        " [  0.    0.    0.   64.    0.    0. ]\n",
        " [  0.   80.   51.2   0.   80.    0. ]\n",
        " [  0.   80.   51.2   0.    0.  100. ]\n",
        " [  0.   80.    0.    0.   80.  100. ]]\n",
        "#Testing\n",
        "#goal_state =5\n",
        "current_state =1\n",
        "steps=[current_state]\n",
        "\n",
        "while current_state != 5:\n",
        "  next_step_index=np.where(Q[current_state,] == np.max(Q[current_state],))[1]\n",
        "  if next_step_index.shape[0]>1:\n",
        "    next_step_index=int(np.random.choice(next_step_index,size=1))\n",
        "  else:\n",
        "    next_step_index=int(next_step_index)\n",
        "\n",
        "  steps.append(next_step_index)\n",
        "  current_state = next_step_index\n",
        "#print selected steps\n",
        "steps\n",
        "[1, 5]\n",
        "Q\n",
        "matrix([[  0.,   0.,   0.,   0., 400.,   0.],\n",
        "        [  0.,   0.,   0., 320.,   0., 500.],\n",
        "        [  0.,   0.,   0., 320.,   0.,   0.],\n",
        "        [  0., 400., 256.,   0., 400.,   0.],\n",
        "        [  0., 400., 256.,   0.,   0., 500.],\n",
        "        [  0., 400.,   0.,   0., 400., 500.]])\n"
      ]
    }
  ]
}